{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert4Rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постановка задачи: В последовательных рекомендательных системах мы прогнозируем следующий элемент (например, фильм) для пользователя на основе последовательности его прошлых взаимодействий. То есть у каждого пользователя имеется упорядоченный список просмотренных фильмов (история), и система должна рекомендовать, что он посмотрит дальше. Традиционные подходы к последовательным рекомендациям используют рекуррентные нейросети или однонаправленные трансформеры, которые просматривают историю слева направо, предсказывая следующий элемент по предыдущим. Однако такие однонаправленные модели имеют ограничения: (a) они учитывают только левый контекст (прошлое) и игнорируют правый контекст, что может приводить к неоптимальному представлению последовательности; (b) они жёстко предполагают строго упорядоченную последовательность, что не всегда соответствует реальности поведения пользователей\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выбор модели: Для устранения этих ограничений была предложена модель BERT4Rec – Bidirectional Encoder Representations from Transformers for Sequential Recommendation. По аналогии с BERT в NLP, BERT4Rec применяет глубокое двунаправленное самовнимание (Transformer) для моделирования последовательности действий пользователя. В отличие от однонаправленных моделей (например, SASRec на основе Transformer Decoder), BERT4Rec одновременно учитывает и левый, и правый контекст каждого элемента в истории. Это позволяет каждому элементу истории «впитать» информацию от соседей с обеих сторон, формируя более информативное представление предпочтений пользователя. Таким образом, модель не делает жёсткого предположения о порядке и может улавливать произвольные дальние зависимости в последовательности благодаря механизму самовнимания. BERT4Rec демонстрирует устойчиво высокие результаты качества рекомендаций по сравнению с предыдущими моделями последовательных рекомендаций\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ключевая идея – предсказание маскированных элементов: Прямое обучение двунаправленной модели сталкивается с проблемой тривиальности – если модель увидит всю последовательность, включая будущие элементы, то предсказывать следующий не имеет смысла. Поэтому в BERT4Rec применяется задача Cloze (маскированного моделирования), аналогичная masked language modeling в BERT. Во время тренировки некоторые элементы последовательности случайно маскируются специальным токеном [MASK], и модель учится восстанавливать маскированные позиции по их левому и правому контексту. Например, последовательность из пяти фильмов [v1, v2, v3, v4, v5] может быть преобразована в [v1, [mask], v3, [mask], v5], и модель должна предсказать, что скрывается за первым [mask] (это v2) и за вторым [mask] (v4). Поскольку маскирование может затрагивать несколько позиций, одна последовательность даёт сразу несколько обучающих примеров, что делает обучение эффективнее, чем предсказание только следующего элемента\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инференс (рекомендации): После обучения модель способна генерировать персональные рекомендации. Чтобы предсказать следующий фильм для пользователя, к концy известной истории пользователя добавляют токен [MASK] и пропускают последовательность через модель. Модель, опираясь на двунаправленный контекст, выдаёт распределение вероятностей на месте этого маскированного токена – какие фильмы наиболее вероятно окажутся следующими. Рекомендуется фильм с наивысшей вероятностью, а для метрик качества обычно рассматривают топ-K (например, топ-10) наиболее вероятных кандидатов. Такой подход позволяет непосредственно использовать обученную двунаправленную модель для предсказания будущих взаимодействий пользователя\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rectools.metrics import Recall, Precision, NDCG, calc_metrics\n",
    "from rectools.dataset import Dataset as RecDataset\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════════════╗\n",
    "# ║                         МОДЕЛЬ BERT4REC                               ║\n",
    "# ╚═══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "class BERT4Rec(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT4Rec - адаптация архитектуры BERT для задачи последовательных рекомендаций.\n",
    "    \n",
    "    Основная идея: используем bidirectional self-attention для понимания контекста\n",
    "    последовательности действий пользователя и предсказания замаскированных товаров.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_items, max_seq_len=50, embed_dim=64, num_heads=2,\n",
    "                 hidden_dim=256, num_layers=2, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Инициализация модели BERT4Rec\n",
    "        \n",
    "        Параметры:\n",
    "        ----------\n",
    "        num_items : int\n",
    "            Количество уникальных товаров в каталоге\n",
    "        max_seq_len : int\n",
    "            Максимальная длина последовательности действий пользователя\n",
    "        embed_dim : int  \n",
    "            Размерность эмбеддингов (векторных представлений)\n",
    "        num_heads : int\n",
    "            Количество голов внимания в multi-head attention\n",
    "        hidden_dim : int\n",
    "            Размерность скрытого слоя в feedforward network\n",
    "        num_layers : int\n",
    "            Количество слоев трансформера\n",
    "        dropout : float\n",
    "            Вероятность отключения нейронов для регуляризации\n",
    "        \"\"\"\n",
    "        super(BERT4Rec, self).__init__()\n",
    "\n",
    "        # ===== СЛОЙ ЭМБЕДДИНГОВ ТОВАРОВ =====\n",
    "        self.item_embedding = nn.Embedding(num_items + 2, embed_dim, padding_idx=0)\n",
    "        \"\"\"\n",
    "        Таблица эмбеддингов размера (num_items + 2) × embed_dim\n",
    "        \n",
    "        Индексация:\n",
    "        - 0: [PAD] токен (padding) - всегда нулевой вектор\n",
    "        - 1...num_items: реальные товары из каталога\n",
    "        - num_items + 1: [MASK] токен для маскированного обучения\n",
    "        \n",
    "        padding_idx=0 гарантирует, что эмбеддинг PAD токена не обновляется при обучении\n",
    "        \"\"\"\n",
    "        \n",
    "        # ===== ПОЗИЦИОННЫЕ ЭМБЕДДИНГИ =====\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        Таблица позиционных эмбеддингов max_seq_len × embed_dim\n",
    "        \n",
    "        Зачем нужны:\n",
    "        - Трансформер не имеет встроенного понимания порядка элементов\n",
    "        - Позиционные эмбеддинги добавляют информацию о позиции каждого товара\n",
    "        - Кодирует позиции от 0 до max_seq_len-1\n",
    "        - Позволяет модели различать \"купил вначале\" и \"купил в конце\"\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== ЭНКОДЕР ТРАНСФОРМЕРА =====\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \"\"\"\n",
    "        Стек из num_layers слоев трансформера\n",
    "        \n",
    "        Каждый слой содержит:\n",
    "        1. Multi-head self-attention (с num_heads головами)\n",
    "           - Позволяет каждому товару \"смотреть\" на все остальные\n",
    "        2. Layer normalization\n",
    "        3. Position-wise feedforward network:\n",
    "           - Linear(embed_dim → hidden_dim)\n",
    "           - GELU activation\n",
    "           - Linear(hidden_dim → embed_dim)\n",
    "        4. Еще один layer normalization\n",
    "        5. Residual connections вокруг обоих подслоев\n",
    "        \n",
    "        GELU (Gaussian Error Linear Unit) - более гладкая версия ReLU\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== ВЫХОДНОЙ СЛОЙ =====\n",
    "        self.output_layer = nn.Linear(embed_dim, num_items + 1)\n",
    "        \"\"\"\n",
    "        Преобразует скрытые представления в логиты для каждого товара\n",
    "        \n",
    "        - Входная размерность: embed_dim\n",
    "        - Выходная размерность: num_items + 1 (без PAD токена)\n",
    "        - Для каждой позиции предсказывает scores всех возможных товаров\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Прямой проход модели\n",
    "        \n",
    "        Параметры:\n",
    "        ----------\n",
    "        input_ids : torch.Tensor\n",
    "            Последовательности товаров, shape: [batch_size, seq_len]\n",
    "            \n",
    "        Возвращает:\n",
    "        -----------\n",
    "        logits : torch.Tensor\n",
    "            Логиты для каждой позиции, shape: [batch_size, seq_len, num_items+1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Получаем device и размеры\n",
    "        device = input_ids.device\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # ===== СОЗДАНИЕ ПОЗИЦИОННЫХ ИНДЕКСОВ =====\n",
    "        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        Пошагово:\n",
    "        1. torch.arange(seq_len) → [0, 1, 2, ..., seq_len-1]\n",
    "        2. .unsqueeze(0) → [[0, 1, 2, ...]] (добавляем batch dimension)\n",
    "        3. .expand(batch_size, seq_len) → копируем для каждого примера в батче\n",
    "        \n",
    "        Результат: матрица позиций для всего батча\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== КОМБИНИРОВАНИЕ ЭМБЕДДИНГОВ =====\n",
    "        x = self.item_embedding(input_ids) + self.position_embedding(positions)\n",
    "        \"\"\"\n",
    "        Складываем два типа эмбеддингов:\n",
    "        - Эмбеддинги товаров: \"что это за товар?\"\n",
    "        - Позиционные эмбеддинги: \"где он находится в последовательности?\"\n",
    "        \n",
    "        Результат: [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== МАСКА ПАДДИНГА =====\n",
    "        padding_mask = (input_ids == 0)\n",
    "        \"\"\"\n",
    "        Булева маска: True = позиция с PAD токеном\n",
    "        Трансформер будет игнорировать эти позиции при вычислении attention\n",
    "        Это важно для корректной работы с последовательностями разной длины\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== ПРОХОЖДЕНИЕ ЧЕРЕЗ ЭНКОДЕР =====\n",
    "        x = self.encoder(x, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # ===== ГЕНЕРАЦИЯ ЛОГИТОВ =====\n",
    "        logits = self.output_layer(x)  # [batch_size, seq_len, num_items+1]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════════════╗\n",
    "# ║                    ФУНКЦИИ МАСКИРОВАНИЯ И DATASET                     ║\n",
    "# ╚═══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "def mask_sequence(sequence, mask_token, mask_ratio=0.2):\n",
    "    \"\"\"\n",
    "    Маскирование последовательности для обучения в стиле BERT\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    sequence : list\n",
    "        Исходная последовательность товаров\n",
    "    mask_token : int\n",
    "        Индекс MASK токена\n",
    "    mask_ratio : float\n",
    "        Доля элементов для маскирования (по умолчанию 20%)\n",
    "        \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    input_seq : list\n",
    "        Последовательность с замаскированными элементами\n",
    "    target_seq : list\n",
    "        Целевые значения (-100 для немаскированных позиций)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Копируем исходную последовательность\n",
    "    input_seq = sequence.copy()\n",
    "    \n",
    "    # Инициализируем target: -100 означает \"игнорировать при расчете loss\"\n",
    "    target_seq = [-100] * len(sequence)\n",
    "\n",
    "    # ===== ВЫБОР ПОЗИЦИЙ ДЛЯ МАСКИРОВАНИЯ =====\n",
    "    n_mask = max(1, int(len(sequence) * mask_ratio))  # Минимум 1 маска\n",
    "    mask_indices = np.random.choice(len(sequence), n_mask, replace=False)\n",
    "    \"\"\"\n",
    "    Случайно выбираем n_mask позиций без повторений\n",
    "    Это симулирует задачу: \"угадай, какой товар был на этой позиции\"\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== ПРИМЕНЕНИЕ МАСКИРОВАНИЯ =====\n",
    "    for idx in mask_indices:\n",
    "        target_seq[idx] = sequence[idx]  # Сохраняем оригинальный товар как target\n",
    "        input_seq[idx] = mask_token      # Заменяем на MASK в input\n",
    "    \"\"\"\n",
    "    Модель будет видеть MASK и пытаться предсказать, \n",
    "    какой товар был на этом месте, используя контекст\n",
    "    \"\"\"\n",
    "\n",
    "    return input_seq, target_seq\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset для последовательностей действий пользователей\n",
    "    \n",
    "    Особенности:\n",
    "    - Автоматически обрезает длинные последовательности\n",
    "    - Применяет маскирование для каждого примера\n",
    "    - Добавляет padding для выравнивания длины\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, user_seqs, mask_token, max_len):\n",
    "        \"\"\"\n",
    "        Параметры:\n",
    "        ----------\n",
    "        user_seqs : dict\n",
    "            Словарь {user_id: [item1, item2, ...]}\n",
    "        mask_token : int\n",
    "            Индекс MASK токена\n",
    "        max_len : int\n",
    "            Максимальная длина последовательности\n",
    "        \"\"\"\n",
    "        self.user_seqs = list(user_seqs.values())  # Преобразуем в список\n",
    "        self.mask_token = mask_token\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Количество пользователей в датасете\"\"\"\n",
    "        return len(self.user_seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Получение одного примера для обучения\n",
    "        \n",
    "        Возвращает:\n",
    "        -----------\n",
    "        input_seq : torch.Tensor\n",
    "            Последовательность с масками и padding\n",
    "        target_seq : torch.Tensor  \n",
    "            Целевые значения для маскированных позиций\n",
    "        \"\"\"\n",
    "        \n",
    "        # ===== ОБРЕЗКА ПОСЛЕДОВАТЕЛЬНОСТИ =====\n",
    "        seq = self.user_seqs[idx][-self.max_len:]\n",
    "        \"\"\"\n",
    "        Берем последние max_len элементов\n",
    "        Это сохраняет самую свежую историю пользователя\n",
    "        \"\"\"\n",
    "        \n",
    "        # ===== МАСКИРОВАНИЕ =====\n",
    "        input_seq, target_seq = mask_sequence(seq, self.mask_token)\n",
    "        \n",
    "        # ===== PADDING =====\n",
    "        pad_len = self.max_len - len(input_seq)\n",
    "        input_seq = [0] * pad_len + input_seq      # PAD токены в начало\n",
    "        target_seq = [-100] * pad_len + target_seq  # -100 для игнорирования\n",
    "        \"\"\"\n",
    "        Выравниваем все последовательности до max_len\n",
    "        Padding добавляется в начало (left padding)\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════════════╗\n",
    "# ║                         ФУНКЦИЯ ОБУЧЕНИЯ                              ║\n",
    "# ╚═══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    \"\"\"\n",
    "    Одна эпоха обучения модели\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    model : BERT4Rec\n",
    "        Модель для обучения\n",
    "    dataloader : DataLoader\n",
    "        Загрузчик данных с батчами\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        Оптимизатор (например, Adam)\n",
    "    device : torch.device\n",
    "        Устройство для вычислений (CPU/GPU)\n",
    "        \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    avg_loss : float\n",
    "        Средний loss за эпоху\n",
    "    \"\"\"\n",
    "    \n",
    "    # Переводим модель в режим обучения (включает dropout)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # ===== LOSS ФУНКЦИЯ =====\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss с ignore_index=-100:\n",
    "    - Вычисляет loss только для маскированных позиций\n",
    "    - Игнорирует позиции с target=-100 (немаскированные и padding)\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== ЦИКЛ ОБУЧЕНИЯ =====\n",
    "    for input_seq, target_seq in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Перемещаем данные на нужное устройство\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        # Обнуляем градиенты от предыдущей итерации\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_seq)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # ===== ВЫЧИСЛЕНИЕ LOSS =====\n",
    "        loss = loss_fn(\n",
    "            logits.view(-1, logits.size(-1)),  # [batch*seq_len, vocab_size]\n",
    "            target_seq.view(-1)                 # [batch*seq_len]\n",
    "        )\n",
    "        \"\"\"\n",
    "        Reshape для loss функции:\n",
    "        - logits: 3D → 2D (объединяем batch и sequence dimensions)\n",
    "        - targets: 2D → 1D\n",
    "        \n",
    "        Loss вычисляется только там, где target != -100\n",
    "        \"\"\"\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Обновление весов\n",
    "        optimizer.step()\n",
    "\n",
    "        # Накапливаем loss для статистики\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Возвращаем средний loss\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════════════╗\n",
    "# ║                    ФУНКЦИЯ ГЕНЕРАЦИИ РЕКОМЕНДАЦИЙ                     ║\n",
    "# ╚═══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "def recommend_top_k(model, user_seqs, k, mask_token, max_len, device):\n",
    "    \"\"\"\n",
    "    Генерация топ-K рекомендаций для каждого пользователя\n",
    "    \n",
    "    Стратегия: добавляем MASK токен в конец истории пользователя\n",
    "    и предсказываем, какой товар должен быть на этом месте\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    model : BERT4Rec\n",
    "        Обученная модель\n",
    "    user_seqs : dict\n",
    "        Словарь {user_id: [item1, item2, ...]}\n",
    "    k : int\n",
    "        Количество рекомендаций для каждого пользователя\n",
    "    mask_token : int\n",
    "        Индекс MASK токена\n",
    "    max_len : int\n",
    "        Максимальная длина последовательности\n",
    "    device : torch.device\n",
    "        Устройство для вычислений\n",
    "        \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    recs_df : pd.DataFrame\n",
    "        DataFrame с колонками ['user', 'item', 'score']\n",
    "    \"\"\"\n",
    "    \n",
    "    # Переводим модель в режим оценки (отключает dropout)\n",
    "    model.eval()\n",
    "    recs = []\n",
    "    \n",
    "    # ===== ОБРАБОТКА КАЖДОГО ПОЛЬЗОВАТЕЛЯ =====\n",
    "    for uid, seq in user_seqs.items():\n",
    "        # Подготовка последовательности\n",
    "        seq = seq[-(max_len - 1):]  # Оставляем место для MASK токена\n",
    "        \"\"\"\n",
    "        Берем последние (max_len-1) товаров\n",
    "        Это нужно, чтобы после добавления MASK длина была ≤ max_len\n",
    "        \"\"\"\n",
    "        \n",
    "        # Padding + добавление MASK в конец\n",
    "        pad_len = max_len - 1 - len(seq)\n",
    "        seq = [0] * pad_len + seq + [mask_token]\n",
    "        \"\"\"\n",
    "        Структура финальной последовательности:\n",
    "        [PAD, PAD, ..., item1, item2, ..., itemN, MASK]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Конвертация в тензор\n",
    "        input_seq = torch.tensor(seq).unsqueeze(0).to(device)  # [1, max_len]\n",
    "\n",
    "        # ===== ИНФЕРЕНС =====\n",
    "        with torch.no_grad():  # Отключаем вычисление градиентов\n",
    "            logits = model(input_seq)     # [1, seq_len, vocab_size]\n",
    "            scores = logits[0, -1]        # Берем логиты последней позиции (MASK)\n",
    "            \"\"\"\n",
    "            scores содержит \"уверенность\" модели для каждого товара\n",
    "            Чем выше score, тем вероятнее этот товар следующий\n",
    "            \"\"\"\n",
    "            \n",
    "            # Находим топ-K товаров\n",
    "            topk_values, topk_indices = torch.topk(scores, k)\n",
    "            topk_items = topk_indices.cpu().tolist()\n",
    "            \n",
    "            # ===== ФОРМИРОВАНИЕ РЕЗУЛЬТАТА =====\n",
    "            for i, item_id in enumerate(topk_items):\n",
    "                recs.append({\n",
    "                    'user': uid,\n",
    "                    'item': item_id,\n",
    "                    'score': scores[item_id].item()\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "\n",
    "# ╔═══════════════════════════════════════════════════════════════════════╗\n",
    "# ║                    ОЦЕНКА МЕТРИК С ПОМОЩЬЮ RECTOOLS                   ║\n",
    "# ╚═══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "def evaluate_with_rectools(recs_df, ground_truth_df):\n",
    "    \"\"\"\n",
    "    Вычисление метрик качества рекомендаций\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    recs_df : pd.DataFrame\n",
    "        Рекомендации с колонками ['user', 'item', 'score']\n",
    "    ground_truth_df : pd.DataFrame\n",
    "        Реальные взаимодействия с колонками ['UserID', 'MovieID']\n",
    "        \n",
    "    Возвращает:\n",
    "    -----------\n",
    "    result : dict\n",
    "        Словарь с вычисленными метриками\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== ПОДГОТОВКА ДАННЫХ =====\n",
    "    rec_dataset = RecDataset(\n",
    "        interactions=ground_truth_df.rename(columns={\n",
    "            'UserID': 'user', \n",
    "            'MovieID': 'item'\n",
    "        }),\n",
    "        user_features_df=None,\n",
    "        item_features_df=None\n",
    "    )\n",
    "    \"\"\"\n",
    "    Создаем объект Dataset из rectools\n",
    "    Переименовываем колонки для совместимости с библиотекой\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== ОПРЕДЕЛЕНИЕ МЕТРИК =====\n",
    "    metrics = [\n",
    "        Recall(k=10),     # Полнота: какая доля релевантных товаров найдена\n",
    "        Precision(k=10),  # Точность: какая доля рекомендаций релевантна\n",
    "        NDCG(k=10)        # Ранжирование: учитывает порядок рекомендаций\n",
    "    ]\n",
    "    \"\"\"\n",
    "    Метрики @ 10:\n",
    "    - Recall@10: из всех товаров, которые понравились пользователю,\n",
    "                 какую долю мы включили в топ-10 рекомендаций?\n",
    "    - Precision@10: из 10 рекомендованных товаров, \n",
    "                    какая доля действительно понравилась пользователю?\n",
    "    - NDCG@10: насколько хорошо мы ранжируем товары?\n",
    "               (более релевантные должны быть выше в списке)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== ВЫЧИСЛЕНИЕ МЕТРИК =====\n",
    "    result = calc_metrics(\n",
    "        metrics=metrics,\n",
    "        recommendations=recs_df,\n",
    "        interactions=rec_dataset.interactions\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Загрузка данных MovieLens...\n",
      "✅ Загружено 3883 фильмов\n",
      "✅ Загружено 1000209 оценок\n",
      "✅ Загружено 6040 пользователей\n",
      "\n",
      "📊 Статистика датасета:\n",
      "Количество пользователей: 6040\n",
      "Количество фильмов: 3706\n",
      "Средняя длина истории: 163.5\n",
      "Максимальная длина истории: 2275\n",
      "\n",
      "✅ После препроцессинга:\n",
      "Количество пользователей: 6040\n",
      "Количество уникальных фильмов: 3255\n",
      "Пример последовательности: [1, 2, 3, 4, 5]...\n",
      "\n",
      "📂 Разделение данных:\n",
      "Train sequences: 6040\n",
      "Validation: 6040\n",
      "Test: 6040\n",
      "\n",
      "🤖 Инициализация BERT4Rec:\n",
      "Количество товаров: 3255\n",
      "MASK token ID: 3256\n",
      "Устройство: cpu\n",
      "\n",
      "🚀 Начинаем обучение...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 48/48 [00:23<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 7.6257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 48/48 [00:23<00:00,  2.03it/s]\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/recsys/lib/python3.10/site-packages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1729647065806/work/aten/src/ATen/NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Loss: 7.4768 | Val Hit@10: 0.0225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 48/48 [00:23<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Loss: 7.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 48/48 [00:23<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Loss: 7.3942 | Val Hit@10: 0.0240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 48/48 [00:24<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Loss: 7.3090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 48/48 [00:24<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Loss: 7.2021 | Val Hit@10: 0.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 48/48 [00:25<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Loss: 7.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 48/48 [00:25<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Loss: 7.0872 | Val Hit@10: 0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 48/48 [00:25<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Loss: 7.0726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 48/48 [00:25<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Loss: 7.0626 | Val Hit@10: 0.0275\n",
      "\n",
      "🎯 Генерация финальных рекомендаций...\n",
      "\n",
      "🎬 Примеры рекомендаций для первых 3 пользователей:\n",
      "\n",
      "👤 Пользователь 1:\n",
      "📜 Последние просмотры:\n",
      "   - How to Make an American Quilt (1995)\n",
      "   - Seven (Se7en) (1995)\n",
      "   - Pocahontas (1995)\n",
      "   - When Night Is Falling (1995)\n",
      "   - Usual Suspects, The (1995)\n",
      "🎯 Рекомендации:\n",
      "   - Juror, The (1996) (score: 3.800)\n",
      "   - Dracula: Dead and Loving It (1995) (score: 3.764)\n",
      "   - Big Green, The (1995) (score: 3.597)\n",
      "   - Georgia (1995) (score: 3.498)\n",
      "   - Lawnmower Man 2: Beyond Cyberspace (1996) (score: 3.468)\n",
      "\n",
      "👤 Пользователь 2:\n",
      "📜 Последние просмотры:\n",
      "   - First Knight (1995)\n",
      "   - Free Willy 2: The Adventure Home (1995)\n",
      "   - Hackers (1995)\n",
      "   - Jeffrey (1995)\n",
      "   - Johnny Mnemonic (1995)\n",
      "🎯 Рекомендации:\n",
      "   - Devil in a Blue Dress (1995) (score: 3.404)\n",
      "   - Nine Months (1995) (score: 3.369)\n",
      "   - Beyond Rangoon (1995) (score: 3.142)\n",
      "   - Awfully Big Adventure, An (1995) (score: 3.130)\n",
      "   - Down Periscope (1996) (score: 3.123)\n",
      "\n",
      "👤 Пользователь 3:\n",
      "📜 Последние просмотры:\n",
      "   - Black Sheep (1996)\n",
      "   - Jupiter's Wife (1994)\n",
      "   - Under Siege 2: Dark Territory (1995)\n",
      "   - Pocahontas (1995)\n",
      "   - Unstrung Heroes (1995)\n",
      "🎯 Рекомендации:\n",
      "   - Dracula: Dead and Loving It (1995) (score: 3.722)\n",
      "   - Juror, The (1996) (score: 3.668)\n",
      "   - Frankie Starlight (1995) (score: 3.529)\n",
      "   - Devil in a Blue Dress (1995) (score: 3.489)\n",
      "   - Georgia (1995) (score: 3.447)\n",
      "\n",
      "💾 Сохранение модели...\n",
      "✅ Модель успешно обучена и сохранена!\n",
      "\n",
      "🆕 Рекомендации для нового пользователя:\n",
      "История: ['Toy Story', 'Star Wars', 'Matrix']\n",
      "Рекомендации:\n",
      "  - Old Lady Who Walked in the Sea, The (Vieille qui marchait dans la mer, La) (1991) (score: 3.526)\n",
      "  - Clerks (1994) (score: 3.430)\n",
      "  - Nothing to Lose (1994) (score: 3.357)\n",
      "  - Carpool (1996) (score: 3.220)\n",
      "  - Blue in the Face (1995) (score: 3.166)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===== ЗАГРУЗКА ДАННЫХ =====\n",
    "print(\"📁 Загрузка данных MovieLens...\")\n",
    "\n",
    "movies = pd.read_csv('data/bert4rec/movies.dat',\n",
    "                     sep='::',\n",
    "                     header=None,\n",
    "                     names=['MovieID', 'Title', 'Genres'],\n",
    "                     engine='python',\n",
    "                     encoding='latin-1')\n",
    "\n",
    "ratings = pd.read_csv('data/bert4rec/ratings.dat',\n",
    "                      sep='::',\n",
    "                      header=None,\n",
    "                      names=['UserID', 'MovieID', 'Rating', 'Timestamp'],\n",
    "                      engine='python')\n",
    "\n",
    "with open('data/bert4rec/train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "print(f\"✅ Загружено {len(movies)} фильмов\")\n",
    "print(f\"✅ Загружено {len(ratings)} оценок\")\n",
    "print(f\"✅ Загружено {len(train_data['train'])} пользователей\\n\")\n",
    "\n",
    "# ===== АНАЛИЗ ДАННЫХ =====\n",
    "print(\"📊 Статистика датасета:\")\n",
    "print(f\"Количество пользователей: {ratings['UserID'].nunique()}\")\n",
    "print(f\"Количество фильмов: {ratings['MovieID'].nunique()}\")\n",
    "print(f\"Средняя длина истории: {np.mean([len(seq) for seq in train_data['train'].values()]):.1f}\")\n",
    "print(f\"Максимальная длина истории: {max([len(seq) for seq in train_data['train'].values()])}\")\n",
    "\n",
    "# ===== ПОДГОТОВКА ДАННЫХ ДЛЯ BERT4REC =====\n",
    "\n",
    "class MovieLensPreprocessor:\n",
    "    \"\"\"Препроцессор для подготовки данных MovieLens к BERT4Rec\"\"\"\n",
    "    \n",
    "    def __init__(self, min_seq_len=5, min_item_freq=5):\n",
    "        self.min_seq_len = min_seq_len\n",
    "        self.min_item_freq = min_item_freq\n",
    "        self.item2id = {}\n",
    "        self.id2item = {}\n",
    "        \n",
    "    def fit_transform(self, sequences):\n",
    "        \"\"\"\n",
    "        Преобразует MovieID в последовательные индексы для модели\n",
    "        \"\"\"\n",
    "        # Подсчет частоты фильмов\n",
    "        item_counts = defaultdict(int)\n",
    "        for seq in sequences.values():\n",
    "            for item in seq:\n",
    "                item_counts[item] += 1\n",
    "        \n",
    "        # Фильтрация редких фильмов\n",
    "        frequent_items = {item for item, count in item_counts.items() \n",
    "                         if count >= self.min_item_freq}\n",
    "        \n",
    "        # Создание маппинга MovieID -> index (начиная с 1, 0 = PAD)\n",
    "        sorted_items = sorted(frequent_items)\n",
    "        self.item2id = {item: idx + 1 for idx, item in enumerate(sorted_items)}\n",
    "        self.id2item = {idx: item for item, idx in self.item2id.items()}\n",
    "        \n",
    "        # Преобразование последовательностей\n",
    "        processed_sequences = {}\n",
    "        for user_id, seq in sequences.items():\n",
    "            # Фильтруем и преобразуем\n",
    "            new_seq = [self.item2id[item] for item in seq \n",
    "                      if item in self.item2id]\n",
    "            \n",
    "            # Оставляем только достаточно длинные последовательности\n",
    "            if len(new_seq) >= self.min_seq_len:\n",
    "                processed_sequences[user_id] = new_seq\n",
    "                \n",
    "        return processed_sequences\n",
    "    \n",
    "    def movie_id_to_title(self, movie_id, movies_df):\n",
    "        \"\"\"Получить название фильма по ID\"\"\"\n",
    "        original_id = self.id2item.get(movie_id, None)\n",
    "        if original_id:\n",
    "            title = movies_df[movies_df['MovieID'] == original_id]['Title'].values\n",
    "            return title[0] if len(title) > 0 else f\"Movie {original_id}\"\n",
    "        return \"Unknown\"\n",
    "\n",
    "# Применяем препроцессинг\n",
    "preprocessor = MovieLensPreprocessor(min_seq_len=5, min_item_freq=10)\n",
    "user_sequences = preprocessor.fit_transform(train_data['train'])\n",
    "\n",
    "print(f\"\\n✅ После препроцессинга:\")\n",
    "print(f\"Количество пользователей: {len(user_sequences)}\")\n",
    "print(f\"Количество уникальных фильмов: {len(preprocessor.item2id)}\")\n",
    "print(f\"Пример последовательности: {list(user_sequences.values())[0][:5]}...\")\n",
    "\n",
    "# ===== РАЗДЕЛЕНИЕ НА TRAIN/VAL/TEST =====\n",
    "\n",
    "def split_sequences_for_bert4rec(sequences, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    Разделяет последовательности для обучения BERT4Rec\n",
    "    \n",
    "    Для каждого пользователя:\n",
    "    - train: вся последовательность для маскированного обучения\n",
    "    - val: последний элемент для валидации\n",
    "    - test: предпоследний элемент для тестирования\n",
    "    \"\"\"\n",
    "    train_seqs = {}\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for user_id, seq in sequences.items():\n",
    "        if len(seq) < 3:  # Нужно минимум 3 элемента\n",
    "            continue\n",
    "            \n",
    "        # Для обучения используем всю последовательность (будем маскировать)\n",
    "        train_seqs[user_id] = seq[:-2]\n",
    "        \n",
    "        # Для валидации - предпоследний элемент\n",
    "        val_data.append({\n",
    "            'user': user_id,\n",
    "            'item': seq[-2],\n",
    "            'history': seq[:-2]\n",
    "        })\n",
    "        \n",
    "        # Для теста - последний элемент\n",
    "        test_data.append({\n",
    "            'user': user_id,\n",
    "            'item': seq[-1],\n",
    "            'history': seq[:-1]\n",
    "        })\n",
    "    \n",
    "    return train_seqs, val_data, test_data\n",
    "\n",
    "train_seqs, val_data, test_data = split_sequences_for_bert4rec(user_sequences)\n",
    "print(f\"\\n📂 Разделение данных:\")\n",
    "print(f\"Train sequences: {len(train_seqs)}\")\n",
    "print(f\"Validation: {len(val_data)}\")\n",
    "print(f\"Test: {len(test_data)}\")\n",
    "\n",
    "# ===== ИНИЦИАЛИЗАЦИЯ И ОБУЧЕНИЕ BERT4REC =====\n",
    "\n",
    "# Параметры модели\n",
    "num_items = len(preprocessor.item2id)\n",
    "mask_token = num_items + 1  # Последний индекс для MASK\n",
    "max_seq_len = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"\\n🤖 Инициализация BERT4Rec:\")\n",
    "print(f\"Количество товаров: {num_items}\")\n",
    "print(f\"MASK token ID: {mask_token}\")\n",
    "print(f\"Устройство: {device}\")\n",
    "\n",
    "# Создаем модель\n",
    "model = BERT4Rec(\n",
    "    num_items=num_items,\n",
    "    max_seq_len=max_seq_len,\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# Создаем датасет и даталоадер\n",
    "dataset = SequenceDataset(\n",
    "    user_seqs=train_seqs,\n",
    "    mask_token=mask_token,\n",
    "    max_len=max_seq_len\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Оптимизатор и планировщик\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# ===== ОБУЧЕНИЕ =====\n",
    "print(\"\\n🚀 Начинаем обучение...\")\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Обучение\n",
    "    avg_loss = train(model, train_loader, optimizer, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Валидация каждые 2 эпохи\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        # Генерируем рекомендации для валидации\n",
    "        val_seqs = {data['user']: data['history'] for data in val_data}\n",
    "        val_recs = recommend_top_k(\n",
    "            model, val_seqs, k=10, \n",
    "            mask_token=mask_token, \n",
    "            max_len=max_seq_len, \n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Подготавливаем ground truth\n",
    "        val_ground_truth = pd.DataFrame([\n",
    "            {'user': data['user'], 'item': data['item']} \n",
    "            for data in val_data\n",
    "        ])\n",
    "        \n",
    "        # Считаем метрики\n",
    "        from rectools.metrics import Recall, Precision, HitRate\n",
    "        \n",
    "        # Простая метрика Hit Rate@10\n",
    "        hits = 0\n",
    "        for data in val_data:\n",
    "            user_recs = val_recs[val_recs['user'] == data['user']]['item'].values\n",
    "            if data['item'] in user_recs:\n",
    "                hits += 1\n",
    "        hit_rate = hits / len(val_data)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Val Hit@10: {hit_rate:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ===== ГЕНЕРАЦИЯ РЕКОМЕНДАЦИЙ =====\n",
    "print(\"\\n🎯 Генерация финальных рекомендаций...\")\n",
    "\n",
    "# Для тестовых пользователей\n",
    "test_seqs = {data['user']: data['history'] for data in test_data}\n",
    "final_recommendations = recommend_top_k(\n",
    "    model, test_seqs, k=10,\n",
    "    mask_token=mask_token,\n",
    "    max_len=max_seq_len,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ===== ПРИМЕРЫ РЕКОМЕНДАЦИЙ =====\n",
    "print(\"\\n🎬 Примеры рекомендаций для первых 3 пользователей:\")\n",
    "\n",
    "for i, user_id in enumerate(list(test_seqs.keys())[:3]):\n",
    "    print(f\"\\n👤 Пользователь {user_id}:\")\n",
    "    \n",
    "    # История просмотров\n",
    "    history = test_seqs[user_id][-5:]  # Последние 5 фильмов\n",
    "    print(\"📜 Последние просмотры:\")\n",
    "    for movie_id in history:\n",
    "        title = preprocessor.movie_id_to_title(movie_id, movies)\n",
    "        print(f\"   - {title}\")\n",
    "    \n",
    "    # Рекомендации\n",
    "    user_recs = final_recommendations[final_recommendations['user'] == user_id]\n",
    "    print(\"🎯 Рекомендации:\")\n",
    "    for _, rec in user_recs.head(5).iterrows():\n",
    "        title = preprocessor.movie_id_to_title(rec['item'], movies)\n",
    "        print(f\"   - {title} (score: {rec['score']:.3f})\")\n",
    "\n",
    "# # ===== СОХРАНЕНИЕ МОДЕЛИ =====\n",
    "# print(\"\\n💾 Сохранение модели...\")\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'preprocessor': preprocessor,\n",
    "#     'num_items': num_items,\n",
    "#     'mask_token': mask_token,\n",
    "#     'max_seq_len': max_seq_len\n",
    "# }, 'bert4rec_movielens_model.pt')\n",
    "\n",
    "# print(\"✅ Модель успешно обучена и сохранена!\")\n",
    "\n",
    "# ===== ФУНКЦИЯ ДЛЯ РЕКОМЕНДАЦИЙ НОВОМУ ПОЛЬЗОВАТЕЛЮ =====\n",
    "\n",
    "def recommend_for_new_user(movie_titles, model, preprocessor, movies_df, k=10):\n",
    "    \"\"\"\n",
    "    Генерирует рекомендации для нового пользователя на основе списка фильмов\n",
    "    \n",
    "    Параметры:\n",
    "    ----------\n",
    "    movie_titles : list\n",
    "        Список названий фильмов, которые смотрел пользователь\n",
    "    \"\"\"\n",
    "    # Находим MovieID по названиям\n",
    "    movie_ids = []\n",
    "    for title in movie_titles:\n",
    "        matches = movies_df[movies_df['Title'].str.contains(title, case=False)]\n",
    "        if not matches.empty:\n",
    "            movie_ids.append(matches.iloc[0]['MovieID'])\n",
    "    \n",
    "    # Преобразуем в индексы модели\n",
    "    sequence = [preprocessor.item2id.get(mid, 0) for mid in movie_ids]\n",
    "    sequence = [idx for idx in sequence if idx > 0]\n",
    "    \n",
    "    if not sequence:\n",
    "        print(\"❌ Не удалось найти фильмы в базе данных\")\n",
    "        return []\n",
    "    \n",
    "    # Генерируем рекомендации\n",
    "    user_seqs = {'new_user': sequence}\n",
    "    recs = recommend_top_k(\n",
    "        model, user_seqs, k=k,\n",
    "        mask_token=mask_token,\n",
    "        max_len=max_seq_len,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Преобразуем обратно в названия\n",
    "    recommendations = []\n",
    "    for _, rec in recs.iterrows():\n",
    "        title = preprocessor.movie_id_to_title(rec['item'], movies_df)\n",
    "        recommendations.append((title, rec['score']))\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Пример использования\n",
    "print(\"\\n🆕 Рекомендации для нового пользователя:\")\n",
    "user_history = ['Toy Story', 'Star Wars', 'Matrix']\n",
    "print(f\"История: {user_history}\")\n",
    "recs = recommend_for_new_user(user_history, model, preprocessor, movies, k=5)\n",
    "print(\"Рекомендации:\")\n",
    "for title, score in recs:\n",
    "    print(f\"  - {title} (score: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate@10: 0.0295\n"
     ]
    }
   ],
   "source": [
    "# Простая оценка без внешних библиотек\n",
    "def simple_evaluate(model, test_data, mask_token, max_seq_len, device, k=10):\n",
    "    \"\"\"Простая оценка с базовыми метриками\"\"\"\n",
    "    \n",
    "    # Генерируем рекомендации\n",
    "    test_seqs = {data['user']: data['history'] for data in test_data}\n",
    "    recommendations = recommend_top_k(\n",
    "        model, test_seqs, k=k,\n",
    "        mask_token=mask_token,\n",
    "        max_len=max_seq_len,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Считаем Hit Rate\n",
    "    hits = 0\n",
    "    for data in test_data:\n",
    "        user_recs = recommendations[recommendations['user'] == data['user']]['item'].values\n",
    "        if data['item'] in user_recs:\n",
    "            hits += 1\n",
    "    \n",
    "    hit_rate = hits / len(test_data)\n",
    "    print(f\"Hit Rate@{k}: {hit_rate:.4f}\")\n",
    "    \n",
    "    return hit_rate\n",
    "\n",
    "# Использование\n",
    "hit_rate = simple_evaluate(model, test_data, mask_token, max_seq_len, device, k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
