{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert4Rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–¥–∞—á–∏: –í –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö –º—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ñ–∏–ª—å–º) –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –µ–≥–æ –ø—Ä–æ—à–ª—ã—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π. –¢–æ –µ—Å—Ç—å —É –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–º–µ–µ—Ç—Å—è —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤ (–∏—Å—Ç–æ—Ä–∏—è), –∏ —Å–∏—Å—Ç–µ–º–∞ –¥–æ–ª–∂–Ω–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å, —á—Ç–æ –æ–Ω –ø–æ—Å–º–æ—Ç—Ä–∏—Ç –¥–∞–ª—å—à–µ. –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –∏–ª–∏ –æ–¥–Ω–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç –∏—Å—Ç–æ—Ä–∏—é —Å–ª–µ–≤–∞ –Ω–∞–ø—Ä–∞–≤–æ, –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—è —Å–ª–µ–¥—É—é—â–∏–π —ç–ª–µ–º–µ–Ω—Ç –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–∏–º. –û–¥–Ω–∞–∫–æ —Ç–∞–∫–∏–µ –æ–¥–Ω–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: (a) –æ–Ω–∏ —É—á–∏—Ç—ã–≤–∞—é—Ç —Ç–æ–ª—å–∫–æ –ª–µ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (–ø—Ä–æ—à–ª–æ–µ) –∏ –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç –ø—Ä–∞–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, —á—Ç–æ –º–æ–∂–µ—Ç –ø—Ä–∏–≤–æ–¥–∏—Ç—å –∫ –Ω–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–º—É –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏; (b) –æ–Ω–∏ –∂—ë—Å—Ç–∫–æ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞—é—Ç —Å—Ç—Ä–æ–≥–æ —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, —á—Ç–æ –Ω–µ –≤—Å–µ–≥–¥–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–≤–µ–¥–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏: –î–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è —ç—Ç–∏—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –±—ã–ª–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å BERT4Rec ‚Äì Bidirectional Encoder Representations from Transformers for Sequential Recommendation. –ü–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å BERT –≤ NLP, BERT4Rec –ø—Ä–∏–º–µ–Ω—è–µ—Ç –≥–ª—É–±–æ–∫–æ–µ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏–µ (Transformer) –¥–ª—è –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç –æ–¥–Ω–æ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, SASRec –Ω–∞ –æ—Å–Ω–æ–≤–µ Transformer Decoder), BERT4Rec –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç –∏ –ª–µ–≤—ã–π, –∏ –ø—Ä–∞–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏–∏. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∫–∞–∂–¥–æ–º—É —ç–ª–µ–º–µ–Ω—Ç—É –∏—Å—Ç–æ—Ä–∏–∏ ¬´–≤–ø–∏—Ç–∞—Ç—å¬ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ—Ç —Å–æ—Å–µ–¥–µ–π —Å –æ–±–µ–∏—Ö —Å—Ç–æ—Ä–æ–Ω, —Ñ–æ—Ä–º–∏—Ä—É—è –±–æ–ª–µ–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è. –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –º–æ–¥–µ–ª—å –Ω–µ –¥–µ–ª–∞–µ—Ç –∂—ë—Å—Ç–∫–æ–≥–æ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏—è –æ –ø–æ—Ä—è–¥–∫–µ –∏ –º–æ–∂–µ—Ç —É–ª–∞–≤–ª–∏–≤–∞—Ç—å –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–µ –¥–∞–ª—å–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è –º–µ—Ö–∞–Ω–∏–∑–º—É —Å–∞–º–æ–≤–Ω–∏–º–∞–Ω–∏—è. BERT4Rec –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ –≤—ã—Å–æ–∫–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è ‚Äì –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤: –ü—Ä—è–º–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è —Å –ø—Ä–æ–±–ª–µ–º–æ–π —Ç—Ä–∏–≤–∏–∞–ª—å–Ω–æ—Å—Ç–∏ ‚Äì –µ—Å–ª–∏ –º–æ–¥–µ–ª—å —É–≤–∏–¥–∏—Ç –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –≤–∫–ª—é—á–∞—è –±—É–¥—É—â–∏–µ —ç–ª–µ–º–µ–Ω—Ç—ã, —Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π –Ω–µ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª–∞. –ü–æ—ç—Ç–æ–º—É –≤ BERT4Rec –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∑–∞–¥–∞—á–∞ Cloze (–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è), –∞–Ω–∞–ª–æ–≥–∏—á–Ω–∞—è masked language modeling –≤ BERT. –í–æ –≤—Ä–µ–º—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–ª—É—á–∞–π–Ω–æ –º–∞—Å–∫–∏—Ä—É—é—Ç—Å—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º —Ç–æ–∫–µ–Ω–æ–º [MASK], –∏ –º–æ–¥–µ–ª—å —É—á–∏—Ç—Å—è –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏ –ø–æ –∏—Ö –ª–µ–≤–æ–º—É –∏ –ø—Ä–∞–≤–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É. –ù–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–∑ –ø—è—Ç–∏ —Ñ–∏–ª—å–º–æ–≤ [v1, v2, v3, v4, v5] –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∞ –≤ [v1, [mask], v3, [mask], v5], –∏ –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ —Å–∫—Ä—ã–≤–∞–µ—Ç—Å—è –∑–∞ –ø–µ—Ä–≤—ã–º [mask] (—ç—Ç–æ v2) –∏ –∑–∞ –≤—Ç–æ—Ä—ã–º [mask] (v4). –ü–æ—Å–∫–æ–ª—å–∫—É –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–∂–µ—Ç –∑–∞—Ç—Ä–∞–≥–∏–≤–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–∑–∏—Ü–∏–π, –æ–¥–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–∞—ë—Ç —Å—Ä–∞–∑—É –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤, —á—Ç–æ –¥–µ–ª–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ, —á–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Ñ–µ—Ä–µ–Ω—Å (—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏): –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª—å —Å–ø–æ—Å–æ–±–Ω–∞ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏. –ß—Ç–æ–±—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —Ñ–∏–ª—å–º –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∫ –∫–æ–Ω—Üy –∏–∑–≤–µ—Å—Ç–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–æ–±–∞–≤–ª—è—é—Ç —Ç–æ–∫–µ–Ω [MASK] –∏ –ø—Ä–æ–ø—É—Å–∫–∞—é—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å. –ú–æ–¥–µ–ª—å, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç, –≤—ã–¥–∞—ë—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –Ω–∞ –º–µ—Å—Ç–µ —ç—Ç–æ–≥–æ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ ‚Äì –∫–∞–∫–∏–µ —Ñ–∏–ª—å–º—ã –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω–æ –æ–∫–∞–∂—É—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–º–∏. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ñ–∏–ª—å–º —Å –Ω–∞–∏–≤—ã—Å—à–µ–π –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é, –∞ –¥–ª—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –æ–±—ã—á–Ω–æ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞—é—Ç —Ç–æ–ø-K (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Ç–æ–ø-10) –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã—Ö –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±—É—á–µ–Ω–Ω—É—é –¥–≤—É–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rectools.metrics import Recall, Precision, NDCG, calc_metrics\n",
    "from rectools.dataset import Dataset as RecDataset\n",
    "\n",
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                         –ú–û–î–ï–õ–¨ BERT4REC                               ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "class BERT4Rec(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT4Rec - –∞–¥–∞–ø—Ç–∞—Ü–∏—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã BERT –¥–ª—è –∑–∞–¥–∞—á–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.\n",
    "    \n",
    "    –û—Å–Ω–æ–≤–Ω–∞—è –∏–¥–µ—è: –∏—Å–ø–æ–ª—å–∑—É–µ–º bidirectional self-attention –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n",
    "    –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_items, max_seq_len=50, embed_dim=64, num_heads=2,\n",
    "                 hidden_dim=256, num_layers=2, dropout=0.1):\n",
    "        \"\"\"\n",
    "        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ BERT4Rec\n",
    "        \n",
    "        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "        ----------\n",
    "        num_items : int\n",
    "            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –≤ –∫–∞—Ç–∞–ª–æ–≥–µ\n",
    "        max_seq_len : int\n",
    "            –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "        embed_dim : int  \n",
    "            –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π)\n",
    "        num_heads : int\n",
    "            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ multi-head attention\n",
    "        hidden_dim : int\n",
    "            –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –≤ feedforward network\n",
    "        num_layers : int\n",
    "            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞\n",
    "        dropout : float\n",
    "            –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–æ–≤ –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
    "        \"\"\"\n",
    "        super(BERT4Rec, self).__init__()\n",
    "\n",
    "        # ===== –°–õ–û–ô –≠–ú–ë–ï–î–î–ò–ù–ì–û–í –¢–û–í–ê–†–û–í =====\n",
    "        self.item_embedding = nn.Embedding(num_items + 2, embed_dim, padding_idx=0)\n",
    "        \"\"\"\n",
    "        –¢–∞–±–ª–∏—Ü–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–∞ (num_items + 2) √ó embed_dim\n",
    "        \n",
    "        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è:\n",
    "        - 0: [PAD] —Ç–æ–∫–µ–Ω (padding) - –≤—Å–µ–≥–¥–∞ –Ω—É–ª–µ–≤–æ–π –≤–µ–∫—Ç–æ—Ä\n",
    "        - 1...num_items: —Ä–µ–∞–ª—å–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã –∏–∑ –∫–∞—Ç–∞–ª–æ–≥–∞\n",
    "        - num_items + 1: [MASK] —Ç–æ–∫–µ–Ω –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "        \n",
    "        padding_idx=0 –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ —ç–º–±–µ–¥–¥–∏–Ω–≥ PAD —Ç–æ–∫–µ–Ω–∞ –Ω–µ –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏\n",
    "        \"\"\"\n",
    "        \n",
    "        # ===== –ü–û–ó–ò–¶–ò–û–ù–ù–´–ï –≠–ú–ë–ï–î–î–ò–ù–ì–ò =====\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        \"\"\"\n",
    "        –¢–∞–±–ª–∏—Ü–∞ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ max_seq_len √ó embed_dim\n",
    "        \n",
    "        –ó–∞—á–µ–º –Ω—É–∂–Ω—ã:\n",
    "        - –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –Ω–µ –∏–º–µ–µ—Ç –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
    "        - –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–æ–±–∞–≤–ª—è—é—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–æ–∑–∏—Ü–∏–∏ –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞\n",
    "        - –ö–æ–¥–∏—Ä—É–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ –æ—Ç 0 –¥–æ max_seq_len-1\n",
    "        - –ü–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ —Ä–∞–∑–ª–∏—á–∞—Ç—å \"–∫—É–ø–∏–ª –≤–Ω–∞—á–∞–ª–µ\" –∏ \"–∫—É–ø–∏–ª –≤ –∫–æ–Ω—Ü–µ\"\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== –≠–ù–ö–û–î–ï–† –¢–†–ê–ù–°–§–û–†–ú–ï–†–ê =====\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \"\"\"\n",
    "        –°—Ç–µ–∫ –∏–∑ num_layers —Å–ª–æ–µ–≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞\n",
    "        \n",
    "        –ö–∞–∂–¥—ã–π —Å–ª–æ–π —Å–æ–¥–µ—Ä–∂–∏—Ç:\n",
    "        1. Multi-head self-attention (—Å num_heads –≥–æ–ª–æ–≤–∞–º–∏)\n",
    "           - –ü–æ–∑–≤–æ–ª—è–µ—Ç –∫–∞–∂–¥–æ–º—É —Ç–æ–≤–∞—Ä—É \"—Å–º–æ—Ç—Ä–µ—Ç—å\" –Ω–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ\n",
    "        2. Layer normalization\n",
    "        3. Position-wise feedforward network:\n",
    "           - Linear(embed_dim ‚Üí hidden_dim)\n",
    "           - GELU activation\n",
    "           - Linear(hidden_dim ‚Üí embed_dim)\n",
    "        4. –ï—â–µ –æ–¥–∏–Ω layer normalization\n",
    "        5. Residual connections –≤–æ–∫—Ä—É–≥ –æ–±–æ–∏—Ö –ø–æ–¥—Å–ª–æ–µ–≤\n",
    "        \n",
    "        GELU (Gaussian Error Linear Unit) - –±–æ–ª–µ–µ –≥–ª–∞–¥–∫–∞—è –≤–µ—Ä—Å–∏—è ReLU\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== –í–´–•–û–î–ù–û–ô –°–õ–û–ô =====\n",
    "        self.output_layer = nn.Linear(embed_dim, num_items + 1)\n",
    "        \"\"\"\n",
    "        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –≤ –ª–æ–≥–∏—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞\n",
    "        \n",
    "        - –í—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: embed_dim\n",
    "        - –í—ã—Ö–æ–¥–Ω–∞—è —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: num_items + 1 (–±–µ–∑ PAD —Ç–æ–∫–µ–Ω–∞)\n",
    "        - –î–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç scores –≤—Å–µ—Ö –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ –º–æ–¥–µ–ª–∏\n",
    "        \n",
    "        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "        ----------\n",
    "        input_ids : torch.Tensor\n",
    "            –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ç–æ–≤–∞—Ä–æ–≤, shape: [batch_size, seq_len]\n",
    "            \n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        -----------\n",
    "        logits : torch.Tensor\n",
    "            –õ–æ–≥–∏—Ç—ã –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–∑–∏—Ü–∏–∏, shape: [batch_size, seq_len, num_items+1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º device –∏ —Ä–∞–∑–º–µ—Ä—ã\n",
    "        device = input_ids.device\n",
    "        batch_size, seq_len = input_ids.size()\n",
    "\n",
    "        # ===== –°–û–ó–î–ê–ù–ò–ï –ü–û–ó–ò–¶–ò–û–ù–ù–´–• –ò–ù–î–ï–ö–°–û–í =====\n",
    "        positions = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        \"\"\"\n",
    "        –ü–æ—à–∞–≥–æ–≤–æ:\n",
    "        1. torch.arange(seq_len) ‚Üí [0, 1, 2, ..., seq_len-1]\n",
    "        2. .unsqueeze(0) ‚Üí [[0, 1, 2, ...]] (–¥–æ–±–∞–≤–ª—è–µ–º batch dimension)\n",
    "        3. .expand(batch_size, seq_len) ‚Üí –∫–æ–ø–∏—Ä—É–µ–º –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –≤ –±–∞—Ç—á–µ\n",
    "        \n",
    "        –†–µ–∑—É–ª—å—Ç–∞—Ç: –º–∞—Ç—Ä–∏—Ü–∞ –ø–æ–∑–∏—Ü–∏–π –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== –ö–û–ú–ë–ò–ù–ò–†–û–í–ê–ù–ò–ï –≠–ú–ë–ï–î–î–ò–ù–ì–û–í =====\n",
    "        x = self.item_embedding(input_ids) + self.position_embedding(positions)\n",
    "        \"\"\"\n",
    "        –°–∫–ª–∞–¥—ã–≤–∞–µ–º –¥–≤–∞ —Ç–∏–ø–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤:\n",
    "        - –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ç–æ–≤–∞—Ä–æ–≤: \"—á—Ç–æ —ç—Ç–æ –∑–∞ —Ç–æ–≤–∞—Ä?\"\n",
    "        - –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏: \"–≥–¥–µ –æ–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏?\"\n",
    "        \n",
    "        –†–µ–∑—É–ª—å—Ç–∞—Ç: [batch_size, seq_len, embed_dim]\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== –ú–ê–°–ö–ê –ü–ê–î–î–ò–ù–ì–ê =====\n",
    "        padding_mask = (input_ids == 0)\n",
    "        \"\"\"\n",
    "        –ë—É–ª–µ–≤–∞ –º–∞—Å–∫–∞: True = –ø–æ–∑–∏—Ü–∏—è —Å PAD —Ç–æ–∫–µ–Ω–æ–º\n",
    "        –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –±—É–¥–µ—Ç –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å —ç—Ç–∏ –ø–æ–∑–∏—Ü–∏–∏ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ attention\n",
    "        –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—è–º–∏ —Ä–∞–∑–Ω–æ–π –¥–ª–∏–Ω—ã\n",
    "        \"\"\"\n",
    "\n",
    "        # ===== –ü–†–û–•–û–ñ–î–ï–ù–ò–ï –ß–ï–†–ï–ó –≠–ù–ö–û–î–ï–† =====\n",
    "        x = self.encoder(x, src_key_padding_mask=padding_mask)\n",
    "        \n",
    "        # ===== –ì–ï–ù–ï–†–ê–¶–ò–Ø –õ–û–ì–ò–¢–û–í =====\n",
    "        logits = self.output_layer(x)  # [batch_size, seq_len, num_items+1]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "\n",
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                    –§–£–ù–ö–¶–ò–ò –ú–ê–°–ö–ò–†–û–í–ê–ù–ò–Ø –ò DATASET                     ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "def mask_sequence(sequence, mask_token, mask_ratio=0.2):\n",
    "    \"\"\"\n",
    "    –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ —Å—Ç–∏–ª–µ BERT\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    sequence : list\n",
    "        –ò—Å—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ç–æ–≤–∞—Ä–æ–≤\n",
    "    mask_token : int\n",
    "        –ò–Ω–¥–µ–∫—Å MASK —Ç–æ–∫–µ–Ω–∞\n",
    "    mask_ratio : float\n",
    "        –î–æ–ª—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20%)\n",
    "        \n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "    -----------\n",
    "    input_seq : list\n",
    "        –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å –∑–∞–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏\n",
    "    target_seq : list\n",
    "        –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (-100 –¥–ª—è –Ω–µ–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π)\n",
    "    \"\"\"\n",
    "    \n",
    "    # –ö–æ–ø–∏—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å\n",
    "    input_seq = sequence.copy()\n",
    "    \n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º target: -100 –æ–∑–Ω–∞—á–∞–µ—Ç \"–∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏ —Ä–∞—Å—á–µ—Ç–µ loss\"\n",
    "    target_seq = [-100] * len(sequence)\n",
    "\n",
    "    # ===== –í–´–ë–û–† –ü–û–ó–ò–¶–ò–ô –î–õ–Ø –ú–ê–°–ö–ò–†–û–í–ê–ù–ò–Ø =====\n",
    "    n_mask = max(1, int(len(sequence) * mask_ratio))  # –ú–∏–Ω–∏–º—É–º 1 –º–∞—Å–∫–∞\n",
    "    mask_indices = np.random.choice(len(sequence), n_mask, replace=False)\n",
    "    \"\"\"\n",
    "    –°–ª—É—á–∞–π–Ω–æ –≤—ã–±–∏—Ä–∞–µ–º n_mask –ø–æ–∑–∏—Ü–∏–π –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π\n",
    "    –≠—Ç–æ —Å–∏–º—É–ª–∏—Ä—É–µ—Ç –∑–∞–¥–∞—á—É: \"—É–≥–∞–¥–∞–π, –∫–∞–∫–æ–π —Ç–æ–≤–∞—Ä –±—ã–ª –Ω–∞ —ç—Ç–æ–π –ø–æ–∑–∏—Ü–∏–∏\"\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== –ü–†–ò–ú–ï–ù–ï–ù–ò–ï –ú–ê–°–ö–ò–†–û–í–ê–ù–ò–Ø =====\n",
    "    for idx in mask_indices:\n",
    "        target_seq[idx] = sequence[idx]  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–æ–≤–∞—Ä –∫–∞–∫ target\n",
    "        input_seq[idx] = mask_token      # –ó–∞–º–µ–Ω—è–µ–º –Ω–∞ MASK –≤ input\n",
    "    \"\"\"\n",
    "    –ú–æ–¥–µ–ª—å –±—É–¥–µ—Ç –≤–∏–¥–µ—Ç—å MASK –∏ –ø—ã—Ç–∞—Ç—å—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, \n",
    "    –∫–∞–∫–æ–π —Ç–æ–≤–∞—Ä –±—ã–ª –Ω–∞ —ç—Ç–æ–º –º–µ—Å—Ç–µ, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç\n",
    "    \"\"\"\n",
    "\n",
    "    return input_seq, target_seq\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–µ–π—Å—Ç–≤–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "    \n",
    "    –û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:\n",
    "    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—Ä–µ–∑–∞–µ—Ç –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "    - –ü—Ä–∏–º–µ–Ω—è–µ—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞\n",
    "    - –î–æ–±–∞–≤–ª—è–µ—Ç padding –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª–∏–Ω—ã\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, user_seqs, mask_token, max_len):\n",
    "        \"\"\"\n",
    "        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "        ----------\n",
    "        user_seqs : dict\n",
    "            –°–ª–æ–≤–∞—Ä—å {user_id: [item1, item2, ...]}\n",
    "        mask_token : int\n",
    "            –ò–Ω–¥–µ–∫—Å MASK —Ç–æ–∫–µ–Ω–∞\n",
    "        max_len : int\n",
    "            –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        \"\"\"\n",
    "        self.user_seqs = list(user_seqs.values())  # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Å–ø–∏—Å–æ–∫\n",
    "        self.mask_token = mask_token\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ\"\"\"\n",
    "        return len(self.user_seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "        \n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "        -----------\n",
    "        input_seq : torch.Tensor\n",
    "            –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å –º–∞—Å–∫–∞–º–∏ –∏ padding\n",
    "        target_seq : torch.Tensor  \n",
    "            –¶–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π\n",
    "        \"\"\"\n",
    "        \n",
    "        # ===== –û–ë–†–ï–ó–ö–ê –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–ò =====\n",
    "        seq = self.user_seqs[idx][-self.max_len:]\n",
    "        \"\"\"\n",
    "        –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ max_len —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
    "        –≠—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–∞–º—É—é —Å–≤–µ–∂—É—é –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "        \"\"\"\n",
    "        \n",
    "        # ===== –ú–ê–°–ö–ò–†–û–í–ê–ù–ò–ï =====\n",
    "        input_seq, target_seq = mask_sequence(seq, self.mask_token)\n",
    "        \n",
    "        # ===== PADDING =====\n",
    "        pad_len = self.max_len - len(input_seq)\n",
    "        input_seq = [0] * pad_len + input_seq      # PAD —Ç–æ–∫–µ–Ω—ã –≤ –Ω–∞—á–∞–ª–æ\n",
    "        target_seq = [-100] * pad_len + target_seq  # -100 –¥–ª—è –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "        \"\"\"\n",
    "        –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –≤—Å–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ max_len\n",
    "        Padding –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –≤ –Ω–∞—á–∞–ª–æ (left padding)\n",
    "        \"\"\"\n",
    "\n",
    "        return torch.tensor(input_seq), torch.tensor(target_seq)\n",
    "\n",
    "\n",
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                         –§–£–ù–ö–¶–ò–Ø –û–ë–£–ß–ï–ù–ò–Ø                              ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    \"\"\"\n",
    "    –û–¥–Ω–∞ —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    model : BERT4Rec\n",
    "        –ú–æ–¥–µ–ª—å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    dataloader : DataLoader\n",
    "        –ó–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö —Å –±–∞—Ç—á–∞–º–∏\n",
    "    optimizer : torch.optim.Optimizer\n",
    "        –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä (–Ω–∞–ø—Ä–∏–º–µ—Ä, Adam)\n",
    "    device : torch.device\n",
    "        –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π (CPU/GPU)\n",
    "        \n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "    -----------\n",
    "    avg_loss : float\n",
    "        –°—Ä–µ–¥–Ω–∏–π loss –∑–∞ —ç–ø–æ—Ö—É\n",
    "    \"\"\"\n",
    "    \n",
    "    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è (–≤–∫–ª—é—á–∞–µ—Ç dropout)\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # ===== LOSS –§–£–ù–ö–¶–ò–Ø =====\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "    \"\"\"\n",
    "    CrossEntropyLoss —Å ignore_index=-100:\n",
    "    - –í—ã—á–∏—Å–ª—è–µ—Ç loss —Ç–æ–ª—å–∫–æ –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π\n",
    "    - –ò–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç –ø–æ–∑–∏—Ü–∏–∏ —Å target=-100 (–Ω–µ–º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏ padding)\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== –¶–ò–ö–õ –û–ë–£–ß–ï–ù–ò–Ø =====\n",
    "    for input_seq, target_seq in tqdm(dataloader, desc=\"Training\"):\n",
    "        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –Ω—É–∂–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ\n",
    "        input_seq = input_seq.to(device)\n",
    "        target_seq = target_seq.to(device)\n",
    "\n",
    "        # –û–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –æ—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_seq)  # [batch_size, seq_len, vocab_size]\n",
    "\n",
    "        # ===== –í–´–ß–ò–°–õ–ï–ù–ò–ï LOSS =====\n",
    "        loss = loss_fn(\n",
    "            logits.view(-1, logits.size(-1)),  # [batch*seq_len, vocab_size]\n",
    "            target_seq.view(-1)                 # [batch*seq_len]\n",
    "        )\n",
    "        \"\"\"\n",
    "        Reshape –¥–ª—è loss —Ñ—É–Ω–∫—Ü–∏–∏:\n",
    "        - logits: 3D ‚Üí 2D (–æ–±—ä–µ–¥–∏–Ω—è–µ–º batch –∏ sequence dimensions)\n",
    "        - targets: 2D ‚Üí 1D\n",
    "        \n",
    "        Loss –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–∞–º, –≥–¥–µ target != -100\n",
    "        \"\"\"\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤\n",
    "        optimizer.step()\n",
    "\n",
    "        # –ù–∞–∫–∞–ø–ª–∏–≤–∞–µ–º loss –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π loss\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                    –§–£–ù–ö–¶–ò–Ø –ì–ï–ù–ï–†–ê–¶–ò–ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô                     ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "def recommend_top_k(model, user_seqs, k, mask_token, max_len, device):\n",
    "    \"\"\"\n",
    "    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–æ–ø-K —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    \n",
    "    –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –¥–æ–±–∞–≤–ª—è–µ–º MASK —Ç–æ–∫–µ–Ω –≤ –∫–æ–Ω–µ—Ü –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º, –∫–∞–∫–æ–π —Ç–æ–≤–∞—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —ç—Ç–æ–º –º–µ—Å—Ç–µ\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    model : BERT4Rec\n",
    "        –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å\n",
    "    user_seqs : dict\n",
    "        –°–ª–æ–≤–∞—Ä—å {user_id: [item1, item2, ...]}\n",
    "    k : int\n",
    "        –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è\n",
    "    mask_token : int\n",
    "        –ò–Ω–¥–µ–∫—Å MASK —Ç–æ–∫–µ–Ω–∞\n",
    "    max_len : int\n",
    "        –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "    device : torch.device\n",
    "        –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
    "        \n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "    -----------\n",
    "    recs_df : pd.DataFrame\n",
    "        DataFrame —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['user', 'item', 'score']\n",
    "    \"\"\"\n",
    "    \n",
    "    # –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º –æ—Ü–µ–Ω–∫–∏ (–æ—Ç–∫–ª—é—á–∞–µ—Ç dropout)\n",
    "    model.eval()\n",
    "    recs = []\n",
    "    \n",
    "    # ===== –û–ë–†–ê–ë–û–¢–ö–ê –ö–ê–ñ–î–û–ì–û –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø =====\n",
    "    for uid, seq in user_seqs.items():\n",
    "        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "        seq = seq[-(max_len - 1):]  # –û—Å—Ç–∞–≤–ª—è–µ–º –º–µ—Å—Ç–æ –¥–ª—è MASK —Ç–æ–∫–µ–Ω–∞\n",
    "        \"\"\"\n",
    "        –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ (max_len-1) —Ç–æ–≤–∞—Ä–æ–≤\n",
    "        –≠—Ç–æ –Ω—É–∂–Ω–æ, —á—Ç–æ–±—ã –ø–æ—Å–ª–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è MASK –¥–ª–∏–Ω–∞ –±—ã–ª–∞ ‚â§ max_len\n",
    "        \"\"\"\n",
    "        \n",
    "        # Padding + –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ MASK –≤ –∫–æ–Ω–µ—Ü\n",
    "        pad_len = max_len - 1 - len(seq)\n",
    "        seq = [0] * pad_len + seq + [mask_token]\n",
    "        \"\"\"\n",
    "        –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:\n",
    "        [PAD, PAD, ..., item1, item2, ..., itemN, MASK]\n",
    "        \"\"\"\n",
    "        \n",
    "        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä\n",
    "        input_seq = torch.tensor(seq).unsqueeze(0).to(device)  # [1, max_len]\n",
    "\n",
    "        # ===== –ò–ù–§–ï–†–ï–ù–° =====\n",
    "        with torch.no_grad():  # –û—Ç–∫–ª—é—á–∞–µ–º –≤—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤\n",
    "            logits = model(input_seq)     # [1, seq_len, vocab_size]\n",
    "            scores = logits[0, -1]        # –ë–µ—Ä–µ–º –ª–æ–≥–∏—Ç—ã –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø–æ–∑–∏—Ü–∏–∏ (MASK)\n",
    "            \"\"\"\n",
    "            scores —Å–æ–¥–µ—Ä–∂–∏—Ç \"—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å\" –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–≤–∞—Ä–∞\n",
    "            –ß–µ–º –≤—ã—à–µ score, —Ç–µ–º –≤–µ—Ä–æ—è—Ç–Ω–µ–µ —ç—Ç–æ—Ç —Ç–æ–≤–∞—Ä —Å–ª–µ–¥—É—é—â–∏–π\n",
    "            \"\"\"\n",
    "            \n",
    "            # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-K —Ç–æ–≤–∞—Ä–æ–≤\n",
    "            topk_values, topk_indices = torch.topk(scores, k)\n",
    "            topk_items = topk_indices.cpu().tolist()\n",
    "            \n",
    "            # ===== –§–û–†–ú–ò–†–û–í–ê–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–ê =====\n",
    "            for i, item_id in enumerate(topk_items):\n",
    "                recs.append({\n",
    "                    'user': uid,\n",
    "                    'item': item_id,\n",
    "                    'score': scores[item_id].item()\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(recs)\n",
    "\n",
    "\n",
    "# ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
    "# ‚ïë                    –û–¶–ï–ù–ö–ê –ú–ï–¢–†–ò–ö –° –ü–û–ú–û–©–¨–Æ RECTOOLS                   ‚ïë\n",
    "# ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
    "\n",
    "def evaluate_with_rectools(recs_df, ground_truth_df):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    recs_df : pd.DataFrame\n",
    "        –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['user', 'item', 'score']\n",
    "    ground_truth_df : pd.DataFrame\n",
    "        –†–µ–∞–ª—å–Ω—ã–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ['UserID', 'MovieID']\n",
    "        \n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:\n",
    "    -----------\n",
    "    result : dict\n",
    "        –°–ª–æ–≤–∞—Ä—å —Å –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• =====\n",
    "    rec_dataset = RecDataset(\n",
    "        interactions=ground_truth_df.rename(columns={\n",
    "            'UserID': 'user', \n",
    "            'MovieID': 'item'\n",
    "        }),\n",
    "        user_features_df=None,\n",
    "        item_features_df=None\n",
    "    )\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç Dataset –∏–∑ rectools\n",
    "    –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π\n",
    "    \"\"\"\n",
    "\n",
    "    # ===== –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö =====\n",
    "    metrics = [\n",
    "        Recall(k=10),     # –ü–æ–ª–Ω–æ—Ç–∞: –∫–∞–∫–∞—è –¥–æ–ª—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞–π–¥–µ–Ω–∞\n",
    "        Precision(k=10),  # –¢–æ—á–Ω–æ—Å—Ç—å: –∫–∞–∫–∞—è –¥–æ–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞\n",
    "        NDCG(k=10)        # –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: —É—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ—Ä—è–¥–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π\n",
    "    ]\n",
    "    \"\"\"\n",
    "    –ú–µ—Ç—Ä–∏–∫–∏ @ 10:\n",
    "    - Recall@10: –∏–∑ –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–Ω—Ä–∞–≤–∏–ª–∏—Å—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é,\n",
    "                 –∫–∞–∫—É—é –¥–æ–ª—é –º—ã –≤–∫–ª—é—á–∏–ª–∏ –≤ —Ç–æ–ø-10 —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π?\n",
    "    - Precision@10: –∏–∑ 10 —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤, \n",
    "                    –∫–∞–∫–∞—è –¥–æ–ª—è –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø–æ–Ω—Ä–∞–≤–∏–ª–∞—Å—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é?\n",
    "    - NDCG@10: –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ö–æ—Ä–æ—à–æ –º—ã —Ä–∞–Ω–∂–∏—Ä—É–µ–º —Ç–æ–≤–∞—Ä—ã?\n",
    "               (–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤—ã—à–µ –≤ —Å–ø–∏—Å–∫–µ)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ===== –í–´–ß–ò–°–õ–ï–ù–ò–ï –ú–ï–¢–†–ò–ö =====\n",
    "    result = calc_metrics(\n",
    "        metrics=metrics,\n",
    "        recommendations=recs_df,\n",
    "        interactions=rec_dataset.interactions\n",
    "    )\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö MovieLens...\n",
      "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 3883 —Ñ–∏–ª—å–º–æ–≤\n",
      "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 1000209 –æ—Ü–µ–Ω–æ–∫\n",
      "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ 6040 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
      "\n",
      "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: 6040\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å–º–æ–≤: 3706\n",
      "–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏: 163.5\n",
      "–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏: 2275\n",
      "\n",
      "‚úÖ –ü–æ—Å–ª–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞:\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: 6040\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤: 3255\n",
      "–ü—Ä–∏–º–µ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: [1, 2, 3, 4, 5]...\n",
      "\n",
      "üìÇ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\n",
      "Train sequences: 6040\n",
      "Validation: 6040\n",
      "Test: 6040\n",
      "\n",
      "ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BERT4Rec:\n",
      "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤: 3255\n",
      "MASK token ID: 3256\n",
      "–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cpu\n",
      "\n",
      "üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:23<00:00,  2.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 7.6257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:23<00:00,  2.03it/s]\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/recsys/lib/python3.10/site-packages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1729647065806/work/aten/src/ATen/NestedTensorImpl.cpp:180.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Loss: 7.4768 | Val Hit@10: 0.0225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:23<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Loss: 7.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:23<00:00,  2.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Loss: 7.3942 | Val Hit@10: 0.0240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:24<00:00,  1.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Loss: 7.3090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:24<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Loss: 7.2021 | Val Hit@10: 0.0276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:25<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Loss: 7.1259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:25<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Loss: 7.0872 | Val Hit@10: 0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:25<00:00,  1.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Loss: 7.0726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 48/48 [00:25<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Loss: 7.0626 | Val Hit@10: 0.0275\n",
      "\n",
      "üéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...\n",
      "\n",
      "üé¨ –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π:\n",
      "\n",
      "üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1:\n",
      "üìú –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã:\n",
      "   - How to Make an American Quilt (1995)\n",
      "   - Seven (Se7en) (1995)\n",
      "   - Pocahontas (1995)\n",
      "   - When Night Is Falling (1995)\n",
      "   - Usual Suspects, The (1995)\n",
      "üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
      "   - Juror, The (1996) (score: 3.800)\n",
      "   - Dracula: Dead and Loving It (1995) (score: 3.764)\n",
      "   - Big Green, The (1995) (score: 3.597)\n",
      "   - Georgia (1995) (score: 3.498)\n",
      "   - Lawnmower Man 2: Beyond Cyberspace (1996) (score: 3.468)\n",
      "\n",
      "üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2:\n",
      "üìú –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã:\n",
      "   - First Knight (1995)\n",
      "   - Free Willy 2: The Adventure Home (1995)\n",
      "   - Hackers (1995)\n",
      "   - Jeffrey (1995)\n",
      "   - Johnny Mnemonic (1995)\n",
      "üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
      "   - Devil in a Blue Dress (1995) (score: 3.404)\n",
      "   - Nine Months (1995) (score: 3.369)\n",
      "   - Beyond Rangoon (1995) (score: 3.142)\n",
      "   - Awfully Big Adventure, An (1995) (score: 3.130)\n",
      "   - Down Periscope (1996) (score: 3.123)\n",
      "\n",
      "üë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 3:\n",
      "üìú –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã:\n",
      "   - Black Sheep (1996)\n",
      "   - Jupiter's Wife (1994)\n",
      "   - Under Siege 2: Dark Territory (1995)\n",
      "   - Pocahontas (1995)\n",
      "   - Unstrung Heroes (1995)\n",
      "üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
      "   - Dracula: Dead and Loving It (1995) (score: 3.722)\n",
      "   - Juror, The (1996) (score: 3.668)\n",
      "   - Frankie Starlight (1995) (score: 3.529)\n",
      "   - Devil in a Blue Dress (1995) (score: 3.489)\n",
      "   - Georgia (1995) (score: 3.447)\n",
      "\n",
      "üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\n",
      "‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!\n",
      "\n",
      "üÜï –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n",
      "–ò—Å—Ç–æ—Ä–∏—è: ['Toy Story', 'Star Wars', 'Matrix']\n",
      "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
      "  - Old Lady Who Walked in the Sea, The (Vieille qui marchait dans la mer, La) (1991) (score: 3.526)\n",
      "  - Clerks (1994) (score: 3.430)\n",
      "  - Nothing to Lose (1994) (score: 3.357)\n",
      "  - Carpool (1996) (score: 3.220)\n",
      "  - Blue in the Face (1995) (score: 3.166)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ===== –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• =====\n",
    "print(\"üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö MovieLens...\")\n",
    "\n",
    "movies = pd.read_csv('data/bert4rec/movies.dat',\n",
    "                     sep='::',\n",
    "                     header=None,\n",
    "                     names=['MovieID', 'Title', 'Genres'],\n",
    "                     engine='python',\n",
    "                     encoding='latin-1')\n",
    "\n",
    "ratings = pd.read_csv('data/bert4rec/ratings.dat',\n",
    "                      sep='::',\n",
    "                      header=None,\n",
    "                      names=['UserID', 'MovieID', 'Rating', 'Timestamp'],\n",
    "                      engine='python')\n",
    "\n",
    "with open('data/bert4rec/train_data.pkl', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(movies)} —Ñ–∏–ª—å–º–æ–≤\")\n",
    "print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(ratings)} –æ—Ü–µ–Ω–æ–∫\")\n",
    "print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(train_data['train'])} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\\n\")\n",
    "\n",
    "# ===== –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–• =====\n",
    "print(\"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞:\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {ratings['UserID'].nunique()}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∏–ª—å–º–æ–≤: {ratings['MovieID'].nunique()}\")\n",
    "print(f\"–°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏: {np.mean([len(seq) for seq in train_data['train'].values()]):.1f}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∏—Å—Ç–æ—Ä–∏–∏: {max([len(seq) for seq in train_data['train'].values()])}\")\n",
    "\n",
    "# ===== –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø BERT4REC =====\n",
    "\n",
    "class MovieLensPreprocessor:\n",
    "    \"\"\"–ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–æ—Ä –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö MovieLens –∫ BERT4Rec\"\"\"\n",
    "    \n",
    "    def __init__(self, min_seq_len=5, min_item_freq=5):\n",
    "        self.min_seq_len = min_seq_len\n",
    "        self.min_item_freq = min_item_freq\n",
    "        self.item2id = {}\n",
    "        self.id2item = {}\n",
    "        \n",
    "    def fit_transform(self, sequences):\n",
    "        \"\"\"\n",
    "        –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç MovieID –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã –¥–ª—è –º–æ–¥–µ–ª–∏\n",
    "        \"\"\"\n",
    "        # –ü–æ–¥—Å—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã —Ñ–∏–ª—å–º–æ–≤\n",
    "        item_counts = defaultdict(int)\n",
    "        for seq in sequences.values():\n",
    "            for item in seq:\n",
    "                item_counts[item] += 1\n",
    "        \n",
    "        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–¥–∫–∏—Ö —Ñ–∏–ª—å–º–æ–≤\n",
    "        frequent_items = {item for item, count in item_counts.items() \n",
    "                         if count >= self.min_item_freq}\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞–ø–ø–∏–Ω–≥–∞ MovieID -> index (–Ω–∞—á–∏–Ω–∞—è —Å 1, 0 = PAD)\n",
    "        sorted_items = sorted(frequent_items)\n",
    "        self.item2id = {item: idx + 1 for idx, item in enumerate(sorted_items)}\n",
    "        self.id2item = {idx: item for item, idx in self.item2id.items()}\n",
    "        \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π\n",
    "        processed_sequences = {}\n",
    "        for user_id, seq in sequences.items():\n",
    "            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º\n",
    "            new_seq = [self.item2id[item] for item in seq \n",
    "                      if item in self.item2id]\n",
    "            \n",
    "            # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏\n",
    "            if len(new_seq) >= self.min_seq_len:\n",
    "                processed_sequences[user_id] = new_seq\n",
    "                \n",
    "        return processed_sequences\n",
    "    \n",
    "    def movie_id_to_title(self, movie_id, movies_df):\n",
    "        \"\"\"–ü–æ–ª—É—á–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∏–ª—å–º–∞ –ø–æ ID\"\"\"\n",
    "        original_id = self.id2item.get(movie_id, None)\n",
    "        if original_id:\n",
    "            title = movies_df[movies_df['MovieID'] == original_id]['Title'].values\n",
    "            return title[0] if len(title) > 0 else f\"Movie {original_id}\"\n",
    "        return \"Unknown\"\n",
    "\n",
    "# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥\n",
    "preprocessor = MovieLensPreprocessor(min_seq_len=5, min_item_freq=10)\n",
    "user_sequences = preprocessor.fit_transform(train_data['train'])\n",
    "\n",
    "print(f\"\\n‚úÖ –ü–æ—Å–ª–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞:\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {len(user_sequences)}\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∏–ª—å–º–æ–≤: {len(preprocessor.item2id)}\")\n",
    "print(f\"–ü—Ä–∏–º–µ—Ä –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {list(user_sequences.values())[0][:5]}...\")\n",
    "\n",
    "# ===== –†–ê–ó–î–ï–õ–ï–ù–ò–ï –ù–ê TRAIN/VAL/TEST =====\n",
    "\n",
    "def split_sequences_for_bert4rec(sequences, val_ratio=0.1, test_ratio=0.1):\n",
    "    \"\"\"\n",
    "    –†–∞–∑–¥–µ–ª—è–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è BERT4Rec\n",
    "    \n",
    "    –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\n",
    "    - train: –≤—Å—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è\n",
    "    - val: –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "    - test: –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    \"\"\"\n",
    "    train_seqs = {}\n",
    "    val_data = []\n",
    "    test_data = []\n",
    "    \n",
    "    for user_id, seq in sequences.items():\n",
    "        if len(seq) < 3:  # –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 3 —ç–ª–µ–º–µ–Ω—Ç–∞\n",
    "            continue\n",
    "            \n",
    "        # –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å (–±—É–¥–µ–º –º–∞—Å–∫–∏—Ä–æ–≤–∞—Ç—å)\n",
    "        train_seqs[user_id] = seq[:-2]\n",
    "        \n",
    "        # –î–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ - –ø—Ä–µ–¥–ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç\n",
    "        val_data.append({\n",
    "            'user': user_id,\n",
    "            'item': seq[-2],\n",
    "            'history': seq[:-2]\n",
    "        })\n",
    "        \n",
    "        # –î–ª—è —Ç–µ—Å—Ç–∞ - –ø–æ—Å–ª–µ–¥–Ω–∏–π —ç–ª–µ–º–µ–Ω—Ç\n",
    "        test_data.append({\n",
    "            'user': user_id,\n",
    "            'item': seq[-1],\n",
    "            'history': seq[:-1]\n",
    "        })\n",
    "    \n",
    "    return train_seqs, val_data, test_data\n",
    "\n",
    "train_seqs, val_data, test_data = split_sequences_for_bert4rec(user_sequences)\n",
    "print(f\"\\nüìÇ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "print(f\"Train sequences: {len(train_seqs)}\")\n",
    "print(f\"Validation: {len(val_data)}\")\n",
    "print(f\"Test: {len(test_data)}\")\n",
    "\n",
    "# ===== –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –ò –û–ë–£–ß–ï–ù–ò–ï BERT4REC =====\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏\n",
    "num_items = len(preprocessor.item2id)\n",
    "mask_token = num_items + 1  # –ü–æ—Å–ª–µ–¥–Ω–∏–π –∏–Ω–¥–µ–∫—Å –¥–ª—è MASK\n",
    "max_seq_len = 100\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"\\nü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è BERT4Rec:\")\n",
    "print(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤: {num_items}\")\n",
    "print(f\"MASK token ID: {mask_token}\")\n",
    "print(f\"–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "model = BERT4Rec(\n",
    "    num_items=num_items,\n",
    "    max_seq_len=max_seq_len,\n",
    "    embed_dim=128,\n",
    "    num_heads=4,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    dropout=0.1\n",
    ").to(device)\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –∏ –¥–∞—Ç–∞–ª–æ–∞–¥–µ—Ä\n",
    "dataset = SequenceDataset(\n",
    "    user_seqs=train_seqs,\n",
    "    mask_token=mask_token,\n",
    "    max_len=max_seq_len\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# ===== –û–ë–£–ß–ï–ù–ò–ï =====\n",
    "print(\"\\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...\")\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # –û–±—É—á–µ–Ω–∏–µ\n",
    "    avg_loss = train(model, train_loader, optimizer, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥—ã–µ 2 —ç–ø–æ—Ö–∏\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "        val_seqs = {data['user']: data['history'] for data in val_data}\n",
    "        val_recs = recommend_top_k(\n",
    "            model, val_seqs, k=10, \n",
    "            mask_token=mask_token, \n",
    "            max_len=max_seq_len, \n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º ground truth\n",
    "        val_ground_truth = pd.DataFrame([\n",
    "            {'user': data['user'], 'item': data['item']} \n",
    "            for data in val_data\n",
    "        ])\n",
    "        \n",
    "        # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "        from rectools.metrics import Recall, Precision, HitRate\n",
    "        \n",
    "        # –ü—Ä–æ—Å—Ç–∞—è –º–µ—Ç—Ä–∏–∫–∞ Hit Rate@10\n",
    "        hits = 0\n",
    "        for data in val_data:\n",
    "            user_recs = val_recs[val_recs['user'] == data['user']]['item'].values\n",
    "            if data['item'] in user_recs:\n",
    "                hits += 1\n",
    "        hit_rate = hits / len(val_data)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | Val Hit@10: {hit_rate:.4f}\")\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# ===== –ì–ï–ù–ï–†–ê–¶–ò–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô =====\n",
    "print(\"\\nüéØ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...\")\n",
    "\n",
    "# –î–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "test_seqs = {data['user']: data['history'] for data in test_data}\n",
    "final_recommendations = recommend_top_k(\n",
    "    model, test_seqs, k=10,\n",
    "    mask_token=mask_token,\n",
    "    max_len=max_seq_len,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# ===== –ü–†–ò–ú–ï–†–´ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô =====\n",
    "print(\"\\nüé¨ –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –ø–µ—Ä–≤—ã—Ö 3 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π:\")\n",
    "\n",
    "for i, user_id in enumerate(list(test_seqs.keys())[:3]):\n",
    "    print(f\"\\nüë§ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {user_id}:\")\n",
    "    \n",
    "    # –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤\n",
    "    history = test_seqs[user_id][-5:]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Ñ–∏–ª—å–º–æ–≤\n",
    "    print(\"üìú –ü–æ—Å–ª–µ–¥–Ω–∏–µ –ø—Ä–æ—Å–º–æ—Ç—Ä—ã:\")\n",
    "    for movie_id in history:\n",
    "        title = preprocessor.movie_id_to_title(movie_id, movies)\n",
    "        print(f\"   - {title}\")\n",
    "    \n",
    "    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "    user_recs = final_recommendations[final_recommendations['user'] == user_id]\n",
    "    print(\"üéØ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\")\n",
    "    for _, rec in user_recs.head(5).iterrows():\n",
    "        title = preprocessor.movie_id_to_title(rec['item'], movies)\n",
    "        print(f\"   - {title} (score: {rec['score']:.3f})\")\n",
    "\n",
    "# # ===== –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ò =====\n",
    "# print(\"\\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...\")\n",
    "# torch.save({\n",
    "#     'model_state_dict': model.state_dict(),\n",
    "#     'preprocessor': preprocessor,\n",
    "#     'num_items': num_items,\n",
    "#     'mask_token': mask_token,\n",
    "#     'max_seq_len': max_seq_len\n",
    "# }, 'bert4rec_movielens_model.pt')\n",
    "\n",
    "# print(\"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞!\")\n",
    "\n",
    "# ===== –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ô –ù–û–í–û–ú–£ –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Æ =====\n",
    "\n",
    "def recommend_for_new_user(movie_titles, model, preprocessor, movies_df, k=10):\n",
    "    \"\"\"\n",
    "    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ø–∏—Å–∫–∞ —Ñ–∏–ª—å–º–æ–≤\n",
    "    \n",
    "    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "    ----------\n",
    "    movie_titles : list\n",
    "        –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —Ñ–∏–ª—å–º–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å–º–æ—Ç—Ä–µ–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å\n",
    "    \"\"\"\n",
    "    # –ù–∞—Ö–æ–¥–∏–º MovieID –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º\n",
    "    movie_ids = []\n",
    "    for title in movie_titles:\n",
    "        matches = movies_df[movies_df['Title'].str.contains(title, case=False)]\n",
    "        if not matches.empty:\n",
    "            movie_ids.append(matches.iloc[0]['MovieID'])\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ –∏–Ω–¥–µ–∫—Å—ã –º–æ–¥–µ–ª–∏\n",
    "    sequence = [preprocessor.item2id.get(mid, 0) for mid in movie_ids]\n",
    "    sequence = [idx for idx in sequence if idx > 0]\n",
    "    \n",
    "    if not sequence:\n",
    "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ñ–∏–ª—å–º—ã –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö\")\n",
    "        return []\n",
    "    \n",
    "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "    user_seqs = {'new_user': sequence}\n",
    "    recs = recommend_top_k(\n",
    "        model, user_seqs, k=k,\n",
    "        mask_token=mask_token,\n",
    "        max_len=max_seq_len,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ –Ω–∞–∑–≤–∞–Ω–∏—è\n",
    "    recommendations = []\n",
    "    for _, rec in recs.iterrows():\n",
    "        title = preprocessor.movie_id_to_title(rec['item'], movies_df)\n",
    "        recommendations.append((title, rec['score']))\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "print(\"\\nüÜï –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –¥–ª—è –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:\")\n",
    "user_history = ['Toy Story', 'Star Wars', 'Matrix']\n",
    "print(f\"–ò—Å—Ç–æ—Ä–∏—è: {user_history}\")\n",
    "recs = recommend_for_new_user(user_history, model, preprocessor, movies, k=5)\n",
    "print(\"–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\")\n",
    "for title, score in recs:\n",
    "    print(f\"  - {title} (score: {score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate@10: 0.0295\n"
     ]
    }
   ],
   "source": [
    "# –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
    "def simple_evaluate(model, test_data, mask_token, max_seq_len, device, k=10):\n",
    "    \"\"\"–ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏\"\"\"\n",
    "    \n",
    "    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "    test_seqs = {data['user']: data['history'] for data in test_data}\n",
    "    recommendations = recommend_top_k(\n",
    "        model, test_seqs, k=k,\n",
    "        mask_token=mask_token,\n",
    "        max_len=max_seq_len,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # –°—á–∏—Ç–∞–µ–º Hit Rate\n",
    "    hits = 0\n",
    "    for data in test_data:\n",
    "        user_recs = recommendations[recommendations['user'] == data['user']]['item'].values\n",
    "        if data['item'] in user_recs:\n",
    "            hits += 1\n",
    "    \n",
    "    hit_rate = hits / len(test_data)\n",
    "    print(f\"Hit Rate@{k}: {hit_rate:.4f}\")\n",
    "    \n",
    "    return hit_rate\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ\n",
    "hit_rate = simple_evaluate(model, test_data, mask_token, max_seq_len, device, k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
